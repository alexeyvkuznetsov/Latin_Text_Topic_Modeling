install.packages(c("aplpack", "foreach", "iterators", "magick", "quanteda", "quantreg", "Rcpp", "reticulate", "sf", "stopwords", "testthat", "tidytext", "xml2"))
library(Rcmdr)
install.packages(c("haven", "Hmisc", "openxlsx", "plotly", "sp"))
install.packages(c("BH", "bit", "cli", "fansi", "farver", "fpc", "hms", "knitr", "leaps", "multcomp", "mvtnorm", "prabclus", "prettyunits", "psych", "qdap", "RcmdrMisc", "RCurl", "Rttf2pt1", "stringi", "tinytex", "wordspace", "xfun", "XML", "zoo"))
install.packages(c("biclust", "bit", "callr", "checkmate", "chron", "dendextend", "digest", "DT", "FactoMineR", "foreach", "fpc", "ggpubr", "ggraph", "hexbin", "Hmisc", "jsonlite", "knitr", "mapproj", "mime", "mnormt", "plotly", "processx", "ps", "RcppArmadillo", "RcppProgress", "rgl", "rstudioapi", "styler", "svglite", "xlsx"))
install.packages("xlsx")
install.packages(c("remotes", "xlsx"))
install.packages(c("ggm", "xlsx"))
install.packages("xlsx")
install.packages(c("covr", "dplyr", "fs", "ggplot2", "lifecycle", "modeltools", "survival", "textdata"))
install.packages(c("Cairo", "car", "crosstalk", "diffobj", "ggraph", "ggrepel", "glue", "graphlayouts", "Hmisc", "igraph", "iotools", "ISOcodes", "lgr", "matrixStats", "nloptr", "quanteda", "rapidjsonr", "Rcpp", "RcppParallel", "roxygen2", "shiny", "svs", "units", "vctrs", "xml2"))
install.packages("igraph")
install.packages(c("arm", "backports", "bigrquery", "broom", "Cairo", "circlize", "dbplyr", "devtools"))
install.packages(c("float", "gargle", "gender", "ggpubr", "git2r"))
install.packages(c("tidyr", "tidyselect", "tidytext", "tinytex", "topicmodels", "vctrs", "withr", "xfun", "xml2", "zoo"))
install.packages("topicmodels")
install.packages(c("modelr", "multcomp", "openxlsx", "pillar", "pkgbuild", "plotrix", "ps", "purrr"))
install.packages("installr")
library(installr)
updateR()
updateR()
install.packages(c("textmineR", "tm", "udpipe"))
install.packages(c("backports", "glue", "maptools"))
install.packages("stm")
install.packages("LSAfun")
install.packages(c("backports", "glmnet", "glue", "maptools"))
install.packages("ggcorrplot")
library(dendextend)
require("igraph")
install.packages(c("backports", "ellipsis", "glmnet", "glue", "maptools", "RcppArmadillo", "RhpcBLASctl"))
install.packages(c("backports", "ellipsis", "glmnet", "RcppArmadillo", "RhpcBLASctl"))
install.packages(c("backports", "car", "carData", "ggraph", "RcppArmadillo", "RhpcBLASctl", "sp", "tidyr", "xfun"))
install.packages(c("backports", "ggraph"))
install.packages("backports")
install.packages(c("backports", "broom", "fs", "ggforce", "ggplot2", "ggpubr", "glmnet", "htmltools", "isoband", "jsonlite", "knitr", "nloptr", "openssl", "pillar", "pkgbuild", "processx", "quanteda", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "rlang", "rstatix", "shiny", "tibble", "tidytext", "vctrs", "xfun"))
install.packages(c("backports", "fs", "ggforce", "RcppArmadillo", "rlang", "vctrs"))
install.packages(c("backports", "RcppArmadillo", "rlang", "vctrs"))
install.packages(c("backports", "car", "data.table", "dplyr", "fs", "httr", "maptools", "ps", "quanteda", "RcppArmadillo", "rlang", "sys", "tidyr", "vctrs", "xfun", "zip"))
install.packages(c("backports", "maptools"))
install.packages(c("backports", "maptools"))
install.packages(c("backports", "callr", "conquer", "cowplot", "dendextend", "glue", "jsonlite", "maptools", "processx", "quantreg", "RcppArmadillo", "stringi", "tidyr", "vctrs", "xfun", "zip"))
install.packages("word2vec")
install.packages(c("jsonlite", "quantreg", "stringi", "xfun"))
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
install.packages("textplot")
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
install.packages(c("backports", "broom", "callr", "car", "cli", "clipr", "coda", "cpp11", "data.table", "digest", "foreach", "htmlwidgets", "igraph", "iterators", "jsonlite", "knitr", "labeling", "lgr", "lme4", "matrixStats", "network", "NLP", "openssl", "openxlsx", "ps", "quanteda", "quantreg", "RcppArmadillo", "readr", "rlang", "shape", "sna", "sp", "statmod", "statnet.common", "stm", "stringi", "tibble", "tidytext", "udpipe", "usethis", "withr", "word2vec", "xfun"))
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
library(udpipe)
library(udpipe)
install.packages(c("backports", "data.table", "gh", "htmlwidgets", "hunspell", "isoband", "ISOcodes", "jsonlite", "lme4", "processx", "ps", "rgl", "rlang", "slam", "stopwords", "syuzhet", "udpipe", "usethis", "word2vec"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages("udpipe")
install.packages("udpipe")
install.packages("installr")
library(installr)
updateR()
install.packages("backports")
install.packages("hunspell")
install.packages("udpipe")
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
setwd("D:/GitHub/Latin_Text_Word2Vec/")
# important option for text analysis
options(stringsAsFactors = F)
# check working directory
getwd()
require(readtext)
data_files <- list.files(path = "files", full.names = T, recursive = T)
# View first 3 files in path
head(data_files, 3)
# docvarsfrom :'arg' should be one of “metadata”, “filenames”, “filepaths”
extracted_texts <- readtext(data_files, docvarsfrom = "filenames", dvsep = "/")
# View first rows of the extracted texts
head(extracted_texts)
# View beginning of the second extracted text
cat(substr(extracted_texts$text[3] , 0, 300))
write.csv2(extracted_texts, file = "data/text_extracts.csv", fileEncoding = "UTF-8")
# clean current workspace
rm(list=ls(all=T))
setwd("D:/GitHub/Latin_Topic_Modeling/")
library(tidyverse)
library(quanteda)
library(stm)
library(tidytext)
library(textstem)
metadata <- readr::read_csv(file = "metadata.csv")
keywords <- metadata %>% filter(tag=="keyword")
View(keywords)
View(metadata)
View(keywords)
View(metadata)
articles <- metadata %>%
select(-source) %>%
filter(!tag == "keyword") %>%
pivot_wider(id_cols = id, names_from = tag, values_from = value) %>%
left_join(keywords %>% count(id) %>% rename(keywords=n)) %>%
mutate(
year=as.integer(year),
cited=as.integer(cited)
) %>%
filter(
year>=2010,
!is.na(abstract)
)
View(articles)
abstracts <- metadata %>%
filter(tag=="abstract") %>%
select(id,value) %>%
left_join(articles %>% select(id,year)) %>% filter(year>=2010)
limpiar <- function(txt) {
txt <- tolower(txt)
txt <- gsub("[^a-z ]","",txt)
txt <- gsub("big data","big_data",txt, fixed = TRUE)
txt <- gsub("â"," ",txt, fixed = TRUE)
txt <- trimws(txt)
return(txt)
}
View(limpiar)
abstracts$value <- limpiar(abstracts$value)
abs_tidy <- abstracts %>%
unnest_tokens(output = word, input = value) %>%
anti_join(stop_words) %>% # remove stopwords
mutate(len = str_length(word)) %>% filter(len>2) %>% select(-len) %>% # remove 1-2 char words
mutate(word=textstem::lemmatize_words(x = word %>% unlist()))
abs_word_keep <- abs_tidy %>% count(id, word, sort = TRUE) %>%
bind_tf_idf(., word, id, `n`) %>%
select(word, tf_idf) %>%
arrange(desc(tf_idf)) %>%
filter(tf_idf > quantile(tf_idf, 0.9, na.rm = T)) %>% pull(word)
abs_dfm <- abs_tidy %>%
filter(word %in% abs_word_keep) %>%
count(id, word, sort = TRUE) %>%
cast_sparse(id, word, n)
View(abs_dfm)
library(furrr)
plan(multiprocess)
many_models <- data_frame(K = c(10, 20, 50)) %>%
mutate(topic_model = future_map(K, ~stm(abs_dfm, K = .,
verbose = TRUE)))
heldout <- make.heldout(abs_dfm)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, abs_dfm),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, abs_dfm),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
k_result %>%
transmute(K,
`Lower bound` = lbound,
Residuals = map_dbl(residual, "dispersion"),
`Semantic coherence` = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics")
View(abs_dfm)
k_result %>%
select(K, exclusivity, semantic_coherence) %>%
unnest() %>%
mutate(K = as.factor(K)) %>%
ggplot(aes(semantic_coherence, exclusivity, color = K)) +
geom_point(size = 2, alpha = 0.7) +
labs(x = "Semantic coherence",
y = "Exclusivity",
title = "Comparing exclusivity and semantic coherence")
# clean current workspace
rm(list=ls(all=T))
# set options
options(stringsAsFactors = F)         # no automatic data transformation
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
prologus<-paste(scan(file ="files/01 prologus.txt",what='character'),collapse=" ")
historia_g<-paste(scan(file ="files/02 historia_g.txt",what='character'),collapse=" ")
recapitulatio<-paste(scan(file ="files/03 recapitulatio.txt",what='character'),collapse=" ")
historia_w<-paste(scan(file ="files/04 historia_w.txt",what='character'),collapse=" ")
historia_s<-paste(scan(file ="files/05 historia_s.txt",what='character'),collapse=" ")
prologus<-data.frame(texts=prologus)
historia_g<-data.frame(texts=historia_g)
recapitulatio<-data.frame(texts=recapitulatio)
historia_w<-data.frame(texts=historia_w)
historia_s<-data.frame(texts=historia_s)
prologus$book<-"01 Prologus"
historia_g$book<-"02 Historia Gothorum"
recapitulatio$book<-"03 Recapitulatio"
historia_w$book<-"04 Historia Wandalorum"
historia_s$book<-"05 Historia Suevorum"
historia<-rbind(prologus,historia_g,recapitulatio,historia_w,historia_s)
#historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
# UDPipe annotation
#udmodel_latin <- udpipe_download_model(language = "latin-ittb")
#udmodel_latin <- udpipe_load_model(ud_model$file_model)
udmodel_latin <- udpipe_load_model(file = "latin-ittb-ud-2.5-191206.udpipe")
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
dtf <- subset(x, upos %in% c("NOUN"))
x <- as.data.frame(x)
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 4)
head(dtm_colsums(dtm))
dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
View(dtm)
library(furrr)
plan(multiprocess)
library(stm)
many_models <- data_frame(K = c(1,10, by=1)) %>%
mutate(topic_model = future_map(K, ~stm(abs_dfm, K = .,
verbose = TRUE)))
many_models <- data_frame(K = c(1,10, by=1)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
many_models <- data_frame(K = c(1,10, by=1)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
many_models <- data_frame(K = c(1,2,3,4,5,6,7,8,9,10)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
y
many_models <- data_frame(K = c(1,2,3,4,5,6,7,8,9,10)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
rlang::last_error()
library(tidyverse)
library(quanteda)
library(stm)
library(tidytext)
library(textstem)
many_models <- data_frame(K = c(1,2,3,4,5,6,7,8,9,10)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
many_models <- data_frame(K = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
many_models <- data_frame(K = c(5, 6, 7, 8, 9, 10, 11)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = FALSE)))
View(many_models)
heldout <- make.heldout(abs_dfm)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dtm),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, dtm),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
heldout <- make.heldout(dtm)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dtm),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, dtm),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
k_result %>%
transmute(K,
`Lower bound` = lbound,
Residuals = map_dbl(residual, "dispersion"),
`Semantic coherence` = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics")
k_result %>%
select(K, exclusivity, semantic_coherence) %>%
unnest() %>%
mutate(K = as.factor(K)) %>%
ggplot(aes(semantic_coherence, exclusivity, color = K)) +
geom_point(size = 2, alpha = 0.7) +
labs(x = "Semantic coherence",
y = "Exclusivity",
title = "Comparing exclusivity and semantic coherence")
topic_model <- k_result %>%
filter(K == 50) %>%
pull(topic_model) %>%
.[[1]]
many_models <- data_frame(K = seq(1, 10, by = 1)) %>%
mutate(topic_model = future_map(K, ~stm(dtm, K = .,
verbose = TRUE)))
View(dtm)
