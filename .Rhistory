install.packages(c("aplpack", "foreach", "iterators", "magick", "quanteda", "quantreg", "Rcpp", "reticulate", "sf", "stopwords", "testthat", "tidytext", "xml2"))
library(Rcmdr)
install.packages(c("haven", "Hmisc", "openxlsx", "plotly", "sp"))
install.packages(c("BH", "bit", "cli", "fansi", "farver", "fpc", "hms", "knitr", "leaps", "multcomp", "mvtnorm", "prabclus", "prettyunits", "psych", "qdap", "RcmdrMisc", "RCurl", "Rttf2pt1", "stringi", "tinytex", "wordspace", "xfun", "XML", "zoo"))
install.packages(c("biclust", "bit", "callr", "checkmate", "chron", "dendextend", "digest", "DT", "FactoMineR", "foreach", "fpc", "ggpubr", "ggraph", "hexbin", "Hmisc", "jsonlite", "knitr", "mapproj", "mime", "mnormt", "plotly", "processx", "ps", "RcppArmadillo", "RcppProgress", "rgl", "rstudioapi", "styler", "svglite", "xlsx"))
install.packages("xlsx")
install.packages(c("remotes", "xlsx"))
install.packages(c("ggm", "xlsx"))
install.packages("xlsx")
install.packages(c("covr", "dplyr", "fs", "ggplot2", "lifecycle", "modeltools", "survival", "textdata"))
install.packages(c("Cairo", "car", "crosstalk", "diffobj", "ggraph", "ggrepel", "glue", "graphlayouts", "Hmisc", "igraph", "iotools", "ISOcodes", "lgr", "matrixStats", "nloptr", "quanteda", "rapidjsonr", "Rcpp", "RcppParallel", "roxygen2", "shiny", "svs", "units", "vctrs", "xml2"))
install.packages("igraph")
install.packages(c("arm", "backports", "bigrquery", "broom", "Cairo", "circlize", "dbplyr", "devtools"))
install.packages(c("float", "gargle", "gender", "ggpubr", "git2r"))
install.packages(c("tidyr", "tidyselect", "tidytext", "tinytex", "topicmodels", "vctrs", "withr", "xfun", "xml2", "zoo"))
install.packages("topicmodels")
install.packages(c("modelr", "multcomp", "openxlsx", "pillar", "pkgbuild", "plotrix", "ps", "purrr"))
install.packages("installr")
library(installr)
updateR()
updateR()
install.packages(c("textmineR", "tm", "udpipe"))
install.packages(c("backports", "glue", "maptools"))
install.packages("stm")
install.packages("LSAfun")
install.packages(c("backports", "glmnet", "glue", "maptools"))
install.packages("ggcorrplot")
library(dendextend)
require("igraph")
install.packages(c("backports", "ellipsis", "glmnet", "glue", "maptools", "RcppArmadillo", "RhpcBLASctl"))
install.packages(c("backports", "ellipsis", "glmnet", "RcppArmadillo", "RhpcBLASctl"))
install.packages(c("backports", "car", "carData", "ggraph", "RcppArmadillo", "RhpcBLASctl", "sp", "tidyr", "xfun"))
install.packages(c("backports", "ggraph"))
install.packages("backports")
install.packages(c("backports", "broom", "fs", "ggforce", "ggplot2", "ggpubr", "glmnet", "htmltools", "isoband", "jsonlite", "knitr", "nloptr", "openssl", "pillar", "pkgbuild", "processx", "quanteda", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "rlang", "rstatix", "shiny", "tibble", "tidytext", "vctrs", "xfun"))
install.packages(c("backports", "fs", "ggforce", "RcppArmadillo", "rlang", "vctrs"))
install.packages(c("backports", "RcppArmadillo", "rlang", "vctrs"))
install.packages(c("backports", "car", "data.table", "dplyr", "fs", "httr", "maptools", "ps", "quanteda", "RcppArmadillo", "rlang", "sys", "tidyr", "vctrs", "xfun", "zip"))
install.packages(c("backports", "maptools"))
install.packages(c("backports", "maptools"))
install.packages(c("backports", "callr", "conquer", "cowplot", "dendextend", "glue", "jsonlite", "maptools", "processx", "quantreg", "RcppArmadillo", "stringi", "tidyr", "vctrs", "xfun", "zip"))
install.packages("word2vec")
install.packages(c("jsonlite", "quantreg", "stringi", "xfun"))
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
install.packages("textplot")
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
install.packages(c("backports", "broom", "callr", "car", "cli", "clipr", "coda", "cpp11", "data.table", "digest", "foreach", "htmlwidgets", "igraph", "iterators", "jsonlite", "knitr", "labeling", "lgr", "lme4", "matrixStats", "network", "NLP", "openssl", "openxlsx", "ps", "quanteda", "quantreg", "RcppArmadillo", "readr", "rlang", "shape", "sna", "sp", "statmod", "statnet.common", "stm", "stringi", "tibble", "tidytext", "udpipe", "usethis", "withr", "word2vec", "xfun"))
library(udpipe)
x <- udpipe("The package provides a dependency parsers built on data from universaldependencies.org", "english")
library(udpipe)
library(udpipe)
install.packages(c("backports", "data.table", "gh", "htmlwidgets", "hunspell", "isoband", "ISOcodes", "jsonlite", "lme4", "processx", "ps", "rgl", "rlang", "slam", "stopwords", "syuzhet", "udpipe", "usethis", "word2vec"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
install.packages("udpipe")
install.packages("udpipe")
install.packages("installr")
library(installr)
updateR()
install.packages("backports")
install.packages("hunspell")
install.packages("udpipe")
install.packages(c("backports", "hunspell", "jsonlite", "udpipe"))
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(stm)
prologus<-paste(scan(file ="files/01 prologus.txt",what='character'),collapse=" ")
historia_g<-paste(scan(file ="files/02 historia_g.txt",what='character'),collapse=" ")
recapitulatio<-paste(scan(file ="files/03 recapitulatio.txt",what='character'),collapse=" ")
historia_w<-paste(scan(file ="files/04 historia_w.txt",what='character'),collapse=" ")
historia_s<-paste(scan(file ="files/05 historia_s.txt",what='character'),collapse=" ")
prologus<-data.frame(texts=prologus)
historia_g<-data.frame(texts=historia_g)
recapitulatio<-data.frame(texts=recapitulatio)
historia_w<-data.frame(texts=historia_w)
historia_s<-data.frame(texts=historia_s)
prologus$book<-"01 Prologus"
historia_g$book<-"02 Historia Gothorum"
recapitulatio$book<-"03 Recapitulatio"
historia_w$book<-"04 Historia Wandalorum"
historia_s$book<-"05 Historia Suevorum"
historia<-rbind(prologus,historia_g,recapitulatio,historia_w,historia_s)
#historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
# UDPipe annotation
#udmodel_latin <- udpipe_download_model(language = "latin-ittb")
#udmodel_latin <- udpipe_load_model(ud_model$file_model)
udmodel_latin <- udpipe_load_model(file = "latin-ittb-ud-2.5-191206.udpipe")
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
x <- as.data.frame(x)
nazi_corp <- x %>%
filter(upos %in% c("NOUN"))
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
nazi_corp <- x %>%
filter(upos %in% c("NOUN"))
nazi_corp <- x %>%  filter(upos %in% c("NOUN"))
nazi_corp <- x %>%  subset(upos %in% c("NOUN"))
nazi_corp <- subset(x, upos %in% c("NOUN"))
View(x)
x <- as.data.frame(x)
nazi_corp <- subset(x, upos %in% c("NOUN"))
dat <- c()
for (i in unique(nazi_corp$doc_id)) {
temp <- nazi_corp %>% filter(doc_id==i)
temp2 <- paste(temp$lemma, collapse=" ")
temp3 <- cbind(i, temp2)
dat <- rbind(dat,temp3)
}
View(nazi_corp)
library(stm)
for (i in unique(nazi_corp$doc_id)) {
temp <- nazi_corp %>% filter(doc_id==i)
temp2 <- paste(temp$lemma, collapse=" ")
temp3 <- cbind(i, temp2)
dat <- rbind(dat,temp3)
}
temp <- nazi_corp %>% filter(nazi_corp$doc_id==i)
for (i in unique(nazi_corp$doc_id)) {
temp <- nazi_corp %>% filter(nazi_corp$doc_id==i)
temp2 <- paste(temp$lemma, collapse=" ")
temp3 <- cbind(i, temp2)
dat <- rbind(dat,temp3)
}
dat <- as_tibble(dat)
??as_tibble
library(tibble)
dat <- as_tibble(dat)
colnames(dat) <- c("docid","text")
View(dat)
# make corpus
nazi_corp <- corpus(dat)
library(tidyverse)
library(stm)
library(tidytext)
library(quanteda)
library(qdap)
library(furrr)
annotated_plots <- x
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
# convert to correct format
nazi_dtm_stm <- convert(dfm, to = "stm")
# finally, run stm
nazi_stm <- stm(documents= nazi_dtm_stm$documents,
vocab=nazi_dtm_stm$vocab,
K = 6,
verbose=F)
plot(nazi_stm,
type = "summary")
plot(nazi_stm,
type = "perspectives",
topics=1:2)
nazi_k <- searchK(nazi_dtm_stm$documents,
nazi_dtm_stm$vocab,
K = 2:10,
verbose=F)
plot(nazi_k)
google_doc_id <- "1LcX-JnpGB0lU1iDnXnxB6WFqBywUKpew" # google file ID
poliblogs<-read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", google_doc_id), stringsAsFactors = FALSE)
processed <- textProcessor(poliblogs$documents, metadata = poliblogs)
View(poliblogs)
View(nazi_stm)
View(poliblogs)
View(nazi_stm)
View(nazi_dtm_stm)
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
x <- as.data.frame(x)
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 4)
dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
# convert to correct format
nazi_dtm_stm <- convert(dtm, to = "stm")
# convert to correct format
nazi_dtm_stm <- convert(dtm, to = "dfm")
View(dtm)
annotated_plots <- x
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
# convert to correct format
nazi_dtm_stm <- convert(dfm, to = "stm")
# Convert a DTM to a Character Vector of documents
library(textmineR)
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 4)
head(dtm_colsums(dtm))
dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
# Convert a DTM to a Character Vector of documents
library(textmineR)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
## Convert dtm to a list of text
dtm.to.docs <- apply(dtm, 1, function(x) {
paste(rep(names(x), x), collapse=" ")
})
myCorpus <- VCorpus(VectorSource(dtm.to.docs))
# make dtm
nazi_dtm <- dfm(myCorpus)
View(myCorpus)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
myCorpus <- VCorpus(VectorSource(dtm.to.docs))
# make dtm
nazi_dtm <- dfm(myCorpus)
# finally, run stm
nazi_stm <- stm(documents= nazi_dtm_stm$documents,
vocab=nazi_dtm_stm$vocab,
K = 4,
verbose=F)
plot(nazi_stm,
type = "summary")
plot(nazi_stm,
type = "summary")
plot(nazi_stm,
type = "perspectives",
topics=1:2)
plot(nazi_stm,
type = "perspectives",
topics=1:4)
plot(nazi_stm,
type = "perspectives",
topics=1:4)
View(nazi_stm)
plot(nazi_stm,
type = "perspectives",
topics=1:4)
# finally, run stm
nazi_stm <- stm(documents= nazi_dtm_stm$documents,
vocab=nazi_dtm_stm$vocab,
K = 4,
verbose=F)
plot(nazi_stm,
type = "summary")
plot(nazi_stm,
type = "summary")
plot(nazi_stm,
type = "perspectives",
topics=1:4)
plot(nazi_stm,
type = "perspectives",
topics=1:2)
plot(nazi_stm,
type = "perspectives",
topics=1:3)
plot(nazi_stm,
type = "perspectives",
topics=1:2)
plot(nazi_stm,
type = "perspectives")
nazi_k <- searchK(nazi_dtm_stm$documents,
nazi_dtm_stm$vocab,
K = 2:10,
verbose=F)
plot(nazi_k)
annotated_plots <- x
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
# convert to correct format
nazi_dtm_stm <- convert(dfm, to = "stm")
# finally, run stm
nazi_stm <- stm(documents= nazi_dtm_stm$documents,
vocab=nazi_dtm_stm$vocab,
K = 4,
verbose=F)
plot(nazi_stm,
type = "summary")
dfm <- dfm %>%
dfm_trim(min_termfreq = 4)
# convert to correct format
nazi_dtm_stm <- convert(dfm, to = "stm")
# finally, run stm
nazi_stm <- stm(documents= nazi_dtm_stm$documents,
vocab=nazi_dtm_stm$vocab,
K = 4,
verbose=F)
plot(nazi_stm,
type = "summary")
