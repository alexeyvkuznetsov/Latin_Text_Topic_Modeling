View(dtf)
View(dtf)
load("historia_annotated_dataset_2_5.Rda")
#set working directory
setwd("D:/GitHub/Latin_Text_LSA/")
require(tm)
require(udpipe)
require(lsa)
require(ggplot2)
require(scatterplot3d)
require(corrplot)
require(factoextra)
load("historia_annotated_dataset_2_5.Rda")
dtf <- subset(x, upos %in% c("ADJ", "ADV", "PROPN", "VERB", "NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
## Create a document-term matrix
dtm <- document_term_matrix(x = dtf)
## Remove words which do not occur that much
dtm <- dtm_remove_lowfreq(dtm, minfreq = 2)
head(dtm_colsums(dtm))
dtm <- dtm_remove_terms(dtm, terms = c("ann", "annus", "aer", "aes", "aera", "num._rom.", "xnum._rom.", "xstincum", "xxnum._rom.", "xxxnum._rom.", "cdxlnum._rom.", "cdlxvus", "cdxcnum._rom.", "cdxcus", "cdxix", "cdxlnum._rom.", "cdxlvium", "cdxlvus", "cdxx", "cdxxcvus", "cdxxxnum._rom.", "clxxnum._rom.", "cxiium", "cxx", "dclix", "dcxliix", "dcxlis", "dcxxnum._rom.", "dcxxxix", "dlxnum._rom.", "dlxxxnum._rom.", "dlxxxvus", "dxnum._rom.", "dxxvus", "obnonum._rom."))
tdm <- t(as.matrix(dtm))
View(tdm)
textDfmlda <- convert(tdm, to = "lda")
View(speeches)
#load("hoc_speeches.Rdata")
load("historia_annotated_dataset.Rda")
## Create a corpus of speeches
lemmaCorpus <- corpus(x$lemma)
View(speeches)
View(x)
## Trim some rarely occuring words
speechDfm <- dfm_trim(speechDfm, min_termfreq = 15, min_docfreq = 0.0015, docfreq_type = "prop")
View(speechDfm)
View(x)
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/")
#load("hoc_speeches.Rdata")
load("historia_annotated_dataset.Rda")
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
# Create a term-document matrix
dtm <- as.matrix(dtm)
tdm <- t(dtm)
View(tdm)
View(dtm)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
# Convert a DTM to a Character Vector of documents
library(textmineR)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
# Create a term-document matrix
dtm <- as.matrix(dtm)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
## Convert dtm to a list of text
dtm.to.docs <- apply(dtm, 1, function(x) {
paste(rep(names(x), x), collapse=" ")
})
myCorpus <- VCorpus(VectorSource(dtm.to.docs))
inspect(myCorpus)
View(myCorpus)
## Convert to dfm, removing some words that appear very regularly
lemmaDfm <- dfm(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
View(tdm)
td_matrix <- as.matrix(tdm)
View(tdm)
View(td_matrix)
View(speechDfm)
View(td_matrix)
load("historia_annotated_dataset.Rda")
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
dtm.to.docs <- textmineR::Dtm2Docs(dtm = dtm)
dtm.to.docs
myCorpus <- VCorpus(VectorSource(dtm.to.docs))
## Convert to dfm, removing some words that appear very regularly
lemmaDfm <- dfm(myCorpus)
myCorpus <- VCorpus(VectorSource(dtm.to.docs))
## Convert to dfm, removing some words that appear very regularly
lemmaDfm <- dfm(myCorpus)
tdm <- TermDocumentMatrix(myCorpus)
View(tdm)
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/")
options(stringsAsFactors = FALSE)
library(quanteda)
require(topicmodels)
textdata <- read.csv("data/sotu.csv", sep = ";", encoding = "UTF-8")
View(textdata)
sotu_corpus <- corpus(x$text, docnames = x$doc_id)
sotu_corpus <- corpus(x$lemma, docnames = x$doc_id)
View(x)
??split
dtf <- subset(x, upos %in% c("NOUN"))
View(dtf)
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
View(dtf)
View(dtf)
View(dtf)
text_prologus <- as.character(select(dtf, term, Liber 1))
text_prologus <- as.character(select(dtf, term))
install.packages("textclean")
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/cloud/")
#clean slate...:
rm(list = ls())
library(udpipe)
Eng_model <- udpipe_download_model(language = "english") #Just specify the language of your data here.
Eng_model <- udpipe_load_model(Eng_model$file_model) #and load to be used in this session
testtext <- read.csv("SmallTest.csv", stringsAsFactors=FALSE)
View(testtext)
z <- udpipe_annotate(Eng_model, x = testtext$text, doc_id = testtext$doc_id)
z <- as.data.frame(z)
View(z)
#Handles twitter better than Shakespeare, apparently.
rm(z)
View(z)
x <- readRDS("annotated_jokes.rds") #pre-annotated our joke sub-sample to save time...
x$topic_level_id <- unique_identifier(x, fields = c("doc_id", "paragraph_id", "sentence_id"))
View(x)
# Build document/term/matrix
dtf <- document_term_frequencies(x, document = "doc_id", term = "lemma") #model on docs.
dtf$term <- gsub(" ", "_", dtf$term, fixed = TRUE) #Whitestrips to underscore
dtf$term <- gsub('\\b\\w{1,2}\\b','',dtf$term) #wordlengts
dtf$term <- gsub('\\b\\w{150,}\\b','',dtf$term) #wordlengts, longer since we have keywords
dtf$term <- gsub(" ", "", dtf$term, fixed = TRUE) #Whitestrips removal
dtf <- dtf[!(is.na(dtf$term) | dtf$term==""), ] #remove empty rows
View(dtf)
dtmKW <- document_term_matrix(x = dtf) #Save as dtm
dtmKW <- dtm_remove_lowfreq(dtmKW, minfreq = 3) #This is the direct method to trim the dtm.
#remodel
library(topicmodels)
#the sampler, as before
controlGibbs <- list(seed = 5683, #what does this mean?
burnin = 200,
iter = 500)
model4 <- LDA(dtmKW, k, method = "Gibbs", control = controlGibbs) #Model...
#remodel
library(topicmodels)
model4 <- LDA(dtmKW, k, method = "Gibbs", control = controlGibbs) #Model...
#remodel
library(topicmodels)
k = 24
#the sampler, as before
controlGibbs <- list(seed = 5683, #what does this mean?
burnin = 200,
iter = 500)
model4 <- LDA(dtmKW, k, method = "Gibbs", control = controlGibbs) #Model...
terms(model4,10) #Looks ok.
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
serVis(topicmodels2LDAvis(model4))
View(dtmKW)
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(text2vec)
load("rome_number_1000.Rda")
#clean slate...:
rm(list = ls())
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(text2vec)
load("historia_annotated_dataset.Rda")
dtf <- subset(x, upos %in% c("NOUN"))
View(dtf)
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
View(dtf)
lda_model <- topicmodels::LDA(dtm, k = 19, method = "Gibbs", control = list(nstart = 5, iter = 4000, burnin = 100, best = TRUE, seed = 1:5, alpha = 0.02))
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
lda_model <- topicmodels::LDA(dtm, k = 19, method = "Gibbs", control = list(nstart = 5, iter = 4000, burnin = 100, best = TRUE, seed = 1:5, alpha = 0.02))
require(LDAvis)
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
serVis(topicmodels2LDAvis(lda_model))
serVis(json, out.dir = "exampleVis", open.browser = TRUE)
serVis(json, out.dir = "Vis", open.browser = TRUE)
###Visualizar####
serVis(topicmodels2LDAvis(lda_model),  out.dir = 'LDAvis', open.browser = interactive())
topics(topicModel)
#clean slate...:
rm(list = ls())
load("historia_annotated_dataset.Rda")
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
head(dtm_colsums(dtm))
lda_model <- topicmodels::LDA(dtm, k = 18, method = "Gibbs", control = list(nstart = 5, iter = 4000, burnin = 100, best = TRUE, seed = 1:5, alpha = 0.02))
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
require(LDAvis)
serVis(topicmodels2LDAvis(lda_model),  out.dir = 'LDAvis', open.browser = interactive())
plot(cars)
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(LDAvis)
load("historia_annotated_dataset.Rda")
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
head(dtm_colsums(dtm))
# ДОРАБОТАТЬ
#dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
#head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 3)
#head(dtm_colsums(dtm))
# ДОРАБОТАТЬ
#dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
#########################################################################
### TOPIC VISUALISATION                                               ###
#########################################################################
library(tidytext)
library(ggplot2)
library(dplyr)
td_beta <- tidy(topicModel)
View(topicmodels2LDAvis)
setwd("D:/GitHub/Gregorius_Turonensis_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(LDAvis)
#########################################################################
### TOPIC VISUALISATION                                               ###
#########################################################################
library(tidytext)
library(ggplot2)
library(dplyr)
topicModel = lda_model
td_beta <- tidy(topicModel)
td_beta %>%
group_by(topic) %>%
top_n(6, beta) %>%
ungroup() %>%
mutate(topic = paste0("Topic ", topic),
term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(x = NULL, y = expression(beta),
title = "Наиболее часто встречающиеся слова для каждой темы")
### Topic proportions https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html
textIds <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
lapply(historia$texts[textIds], as.character)
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(LDAvis)
#t00<-paste(scan(file ="files/00.txt",what='character'),collapse=" ")
t01<-paste(scan(file ="files/01.txt",what='character'),collapse=" ")
t02<-paste(scan(file ="files/02.txt",what='character'),collapse=" ")
t03<-paste(scan(file ="files/03.txt",what='character'),collapse=" ")
t04<-paste(scan(file ="files/04.txt",what='character'),collapse=" ")
t05<-paste(scan(file ="files/05.txt",what='character'),collapse=" ")
t06<-paste(scan(file ="files/06.txt",what='character'),collapse=" ")
t07<-paste(scan(file ="files/07.txt",what='character'),collapse=" ")
t08<-paste(scan(file ="files/08.txt",what='character'),collapse=" ")
t09<-paste(scan(file ="files/09.txt",what='character'),collapse=" ")
t10<-paste(scan(file ="files/10.txt",what='character'),collapse=" ")
#t00<-data.frame(texts=t00)
t01<-data.frame(texts=t01)
t02<-data.frame(texts=t02)
t03<-data.frame(texts=t03)
t04<-data.frame(texts=t04)
t05<-data.frame(texts=t05)
t06<-data.frame(texts=t06)
t07<-data.frame(texts=t07)
t08<-data.frame(texts=t08)
t09<-data.frame(texts=t09)
t10<-data.frame(texts=t10)
#t00$book<-"Praefatio"
t01$book<-"Liber 01"
t02$book<-"Liber 02"
t03$book<-"Liber 03"
t04$book<-"Liber 04"
t05$book<-"Liber 05"
t06$book<-"Liber 06"
t07$book<-"Liber 07"
t08$book<-"Liber 08"
t09$book<-"Liber 09"
t10$book<-"Liber 10"
historia<-rbind(t01,t02,t03,t04,t05,t06,t07,t08,t09,t10)
historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
#########################################################################
### REMOVE STOPWORDS                                                  ###
#########################################################################
#rome_number<-paste(scan(file ="rom number 1000.txt",what='character'),collapse=" ")
#rome_number<-tolower(rome_number)
#rome_number
#write(rome_number, file="rome_number_v.txt")
load("rome_number_1000.Rda")
customStopWords <- c("ann", "annus", "aer", "aes", "aera", "num._rom.", "xnum._rom.", "xxnum._rom.", "xxxnum._rom.", "cdxlnum._rom.")
lat_stop_perseus <- c("ab", "ac", "ad", "adhic", "aliqui", "aliquis", "an", "ante", "apud", "at", "atque", "aut", "autem", "cum", "cur", "de", "deinde", "dum", "ego", "enim", "ergo", "es", "est", "et", "etiam", "etsi", "ex", "fio", "haud", "hic", "iam", "idem", "igitur", "ille", "in", "infra", "inter", "interim", "ipse", "is", "ita", "magis", "modo", "mox", "nam", "ne", "nec", "necque", "neque", "nisi", "non", "nos", "o", "ob", "per", "possum", "post", "pro", "quae", "quam", "quare", "qui", "quia", "quicumque", "quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", "quisquis", "quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", "super", "suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero", "unus", "ut", "quoque", "xiix")
#save(lat_stop_perseus,file="lat_stop_perseus.Rda")
#load("lat_stop_perseus.Rda")
#MyStopwords <- c(lat_stop_perseus, customStopWords, lat_stopwords_romnum)
MyStopwords <- c(lat_stop_perseus, rome_number_1000, customStopWords)
#historia$texts <- removeWords(historia$texts, c(lat_stop_perseus, rome_number_1000))
historia$texts <- removeWords(historia$texts, MyStopwords)
historia$texts <- stripWhitespace(historia$texts)
### Topic proportions https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html
textIds <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
lapply(historia$texts[textIds], as.character)
tmResult <- posterior(topicModel)
topicModel <- lda_model
library(tidytext)
library(ggplot2)
library(dplyr)
td_beta <- tidy(topicModel)
td_beta %>%
group_by(topic) %>%
top_n(6, beta) %>%
ungroup() %>%
mutate(topic = paste0("Topic ", topic),
term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(x = NULL, y = expression(beta),
title = "Наиболее часто встречающиеся слова для каждой темы")
textIds <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
lapply(historia$texts[textIds], as.character)
tmResult <- posterior(topicModel)
theta <- tmResult$topics
beta <- tmResult$terms
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
require(LDAvis)
serVis(topicmodels2LDAvis(lda_model))
#########################################################################
### LDAvis VISUALISATION                                              ###
#########################################################################
# https://github.com/love-borjeson/tm_ws_cloud/blob/master/3_ling_filter.R
# udpipe + LDAvis
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
require(LDAvis)
#serVis(topicmodels2LDAvis(lda_model),  out.dir = 'LDAvis', open.browser = interactive())
serVis(topicmodels2LDAvis(lda_model))
#servr::daemon_stop(1) # to stop the server
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
library(textmineR)
#clean slate...:
rm(list = ls())
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
servr::daemon_stop(1) # to stop the server
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
prologus<-paste(scan(file ="files/01 prologus.txt",what='character'),collapse=" ")
historia_g<-paste(scan(file ="files/02 historia_g.txt",what='character'),collapse=" ")
recapitulatio<-paste(scan(file ="files/03 recapitulatio.txt",what='character'),collapse=" ")
historia_w<-paste(scan(file ="files/04 historia_w.txt",what='character'),collapse=" ")
historia_s<-paste(scan(file ="files/05 historia_s.txt",what='character'),collapse=" ")
prologus<-data.frame(texts=prologus)
historia_g<-data.frame(texts=historia_g)
recapitulatio<-data.frame(texts=recapitulatio)
historia_w<-data.frame(texts=historia_w)
historia_s<-data.frame(texts=historia_s)
prologus$book<-"01 Prologus"
historia_g$book<-"02 Historia Gothorum"
recapitulatio$book<-"03 Recapitulatio"
historia_w$book<-"04 Historia Wandalorum"
historia_s$book<-"05 Historia Suevorum"
historia<-rbind(prologus,historia_g,recapitulatio,historia_w,historia_s)
#historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
# UDPipe annotation
#udmodel_latin <- udpipe_download_model(language = "latin-ittb")
#udmodel_latin <- udpipe_load_model(ud_model$file_model)
udmodel_latin <- udpipe_load_model(file = "latin-ittb-ud-2.5-191206.udpipe")
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
x <- as.data.frame(x)
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 4)
head(dtm_colsums(dtm))
dtm <- dtm_remove_terms(dtm, terms = c("ann.", "ann", "an", "annus", "aer", "aes", "suus", "filius", "pater", "frater", "pars", "maldra", "theudericus", "hucusque", "hispanium", "caeter", "justinianus", "praelio", "cdxxxnum._rom.", "cdxinum._rom.", "cdxix", "op"))
library(topicmodels)
topicModel <- topicmodels::LDA(dtm, k = 4, method = "Gibbs", control = list(nstart = 5, iter = 4000, burnin = 500, best = TRUE, seed = 1:5, alpha = 0.1))
topics(topicModel)
library(tidytext)
library(ggplot2)
library(dplyr)
td_beta <- tidy(topicModel)
td_beta %>%
group_by(topic) %>%
top_n(6, beta) %>%
ungroup() %>%
mutate(topic = paste0("Topic ", topic),
term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(x = NULL, y = expression(beta),
title = "Наиболее часто встречающиеся слова для каждой темы")
textIds <- c(1, 2, 3, 4, 5)
View(historia)
lapply(historia$texts[textIds], as.character)
tmResult <- posterior(topicModel)
textIds <- c(1, 2, 3, 4, 5)
lapply(historia$book[textIds], as.character)
tmResult <- posterior(topicModel)
tmResult <- topicmodels::posterior(topicModel)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel, 7), 2, paste, collapse = " ")
attr(topicModel, "alpha")
library("reshape2")
library("ggplot2")
N <- 5
topicProportionExamples <- theta[textIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
coord_flip() +
facet_wrap(~ document, ncol = N)
