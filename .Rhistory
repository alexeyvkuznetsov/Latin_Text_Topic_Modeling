models_statistics %>%
select(K, exclusivity, semantic_coherence) %>%
unnest(cols = c(exclusivity, semantic_coherence)) %>%
ggplot(aes(x = exclusivity, y = semantic_coherence, color = factor(K))) +
geom_point()
models_statistics %>%
select(K, exclusivity, semantic_coherence) %>%
mutate(
exclusivity = map_dbl(exclusivity, ~mean(unlist(.x))),
semantic_coherence = map_dbl(semantic_coherence, ~mean(unlist(.x)))
)
models_statistics %>%
mutate(
exclusivity = map_dbl(exclusivity, ~mean(unlist(.x))),
semantic_coherence = map_dbl(semantic_coherence, ~mean(unlist(.x))),
heldout = map_dbl(heldout, function(.x) .x[[1]]),
residual = map_dbl(residual, function(.x) .x[[1]])
) %>%
select(K, exclusivity, semantic_coherence, heldout, residual) %>%
View()
models_statistics %>%
mutate(
exclusivity = map_dbl(exclusivity, ~mean(unlist(.x))),
semantic_coherence = map_dbl(semantic_coherence, ~mean(unlist(.x))),
heldout = map_dbl(heldout, function(.x) .x[[1]]),
residual = map_dbl(residual, function(.x) .x[[1]])
) %>%
select(K, exclusivity, semantic_coherence, heldout, residual) %>%
pivot_longer(
exclusivity:residual
) %>%
ggplot(aes(K, value, factor(name))) +
geom_smooth() +
theme_classic() +
facet_wrap(~factor(name), scales = "free")
sageLabels(models$topic_models[3][[1]])
broom::tidy(models$topic_models[3][[1]], "beta") %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col() +
coord_flip() +
facet_wrap(~ factor(topic), scales = 'free')
# clean current workspace
rm(list=ls(all=T))
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(stm)
prologus<-paste(scan(file ="files/01 prologus.txt",what='character'),collapse=" ")
historia_g<-paste(scan(file ="files/02 historia_g.txt",what='character'),collapse=" ")
recapitulatio<-paste(scan(file ="files/03 recapitulatio.txt",what='character'),collapse=" ")
historia_w<-paste(scan(file ="files/04 historia_w.txt",what='character'),collapse=" ")
historia_s<-paste(scan(file ="files/05 historia_s.txt",what='character'),collapse=" ")
prologus<-data.frame(texts=prologus)
historia_g<-data.frame(texts=historia_g)
recapitulatio<-data.frame(texts=recapitulatio)
historia_w<-data.frame(texts=historia_w)
historia_s<-data.frame(texts=historia_s)
prologus$book<-"01 Prologus"
historia_g$book<-"02 Historia Gothorum"
recapitulatio$book<-"03 Recapitulatio"
historia_w$book<-"04 Historia Wandalorum"
historia_s$book<-"05 Historia Suevorum"
historia<-rbind(prologus,historia_g,recapitulatio,historia_w,historia_s)
#historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
# UDPipe annotation
#udmodel_latin <- udpipe_download_model(language = "latin-ittb")
#udmodel_latin <- udpipe_load_model(ud_model$file_model)
udmodel_latin <- udpipe_load_model(file = "latin-ittb-ud-2.5-191206.udpipe")
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
x <- as.data.frame(x)
library(tidyverse)
library(stm)
library(tidytext)
library(quanteda)
library(qdap)
library(furrr)
annotated_plots <- x
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
dfm <- dfm %>%
dfm_trim(min_termfreq = 4)
models <- tibble(
K = seq(6, 24, 3)
) %>%
mutate(
topic_models = future_map(K,
~stm(dfm,
K = .x,
seed = 1000
)
)
)
View(models)
saveRDS(models, "STM_1.RDS")
heldout <- make.heldout(dfm)
models_statistics <- models %>%
mutate(
exclusivity = map(topic_models, exclusivity),
semantic_coherence = map(topic_models, semanticCoherence, dfm),
heldout = map(topic_models, eval.heldout, heldout$missing),
residual = map(topic_models, checkResiduals, dfm)
)
models_statistics %>%
select(K, exclusivity, semantic_coherence) %>%
unnest(cols = c(exclusivity, semantic_coherence)) %>%
ggplot(aes(x = exclusivity, y = semantic_coherence, color = factor(K))) +
geom_point()
models_statistics %>%
select(K, exclusivity, semantic_coherence) %>%
mutate(
exclusivity = map_dbl(exclusivity, ~mean(unlist(.x))),
semantic_coherence = map_dbl(semantic_coherence, ~mean(unlist(.x)))
)
models_statistics %>%
mutate(
exclusivity = map_dbl(exclusivity, ~mean(unlist(.x))),
semantic_coherence = map_dbl(semantic_coherence, ~mean(unlist(.x))),
heldout = map_dbl(heldout, function(.x) .x[[1]]),
residual = map_dbl(residual, function(.x) .x[[1]])
) %>%
select(K, exclusivity, semantic_coherence, heldout, residual) %>%
View()
models <- tibble(
K = seq(1, 10, 1)
) %>%
mutate(
topic_models = future_map(K,
~stm(dfm,
K = .x,
seed = 1000
)
)
)
dfm <- dfm %>%
dfm_trim(min_termfreq = 4)
models <- tibble(
K = seq(1, 10, 1)
) %>%
mutate(
topic_models = future_map(K,
~stm(dfm,
K = .x,
seed = 1000
)
)
)
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
dfm <- dfm %>%
dfm_trim(min_termfreq = 4)
models <- tibble(
K = seq(1, 10, 1)
) %>%
mutate(
topic_models = future_map(K,
~stm(dfm,
K = .x,
seed = 1000
)
)
)
many_models <- data_frame(K = c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)) %>%
mutate(topic_model = future_map(K, ~stm(dfm, K = .,
verbose = FALSE)))
heldout <- make.heldout(ni_tm_sparse)
heldout <- make.heldout(dfm)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dfm),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, dfm),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
many_models <- tibble::tibble(K = seq(1, 10, by = 1)) %>%
dplyr::mutate(topic_model = future_map(K, ~stm(dfm, K = .,
verbose = FALSE)))
many_models <- tibble::tibble(K = seq(2, 10, by = 1)) %>%
dplyr::mutate(topic_model = future_map(K, ~stm(dfm, K = .,
verbose = FALSE)))
k_result <- many_models %>%
dplyr::mutate(exclusivity = purrr::map(topic_model, exclusivity),
semantic_coherence = purrr::map(topic_model, semanticCoherence, threads_sparse),
eval_heldout = purrr::map(topic_model, eval.heldout, heldout$missing),
residual = purrr::map(topic_model, checkResiduals, threads_sparse),
bound =  purrr::map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = purrr::map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = purrr::map_dbl(topic_model, function(x) length(x$convergence$bound)))
library("ggplot2")
k_result %>%
dplyr::transmute(K,
`Lower bound` = lbound,
Residuals = purrr::map_dbl(residual, "dispersion"),
`Semantic coherence` = purrr::map_dbl(semantic_coherence, mean),
`Held-out likelihood` = purrr::map_dbl(eval_heldout, "expected.heldout")) %>%
tidyr::gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics",
subtitle = "We should use domain knowledge to choose a good number of topics :-)") +
hrbrthemes::theme_ipsum(base_size = 16,
axis_title_size = 16)
many_models <- data_frame(K = seq(5, 20, by = 5)) %>% # Here we're running four topic models: 5 topics, 10 topics, 15 topics and 20 topics
mutate(topic_model = future_map(K, ~stm(dfm,
K = .,
verbose = FALSE)))
toc()
DFM <- dfm
t <- Sys.time()
DFM2stm <- convert(DFM, to = "stm")
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab
, K = seq(4, 15, by = 1), max.em.its = 75
, cores = 4
, seed = 9999)
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab
, K = seq(1, 10, by = 1), max.em.its = 75
, seed = 9999)
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab
, K = seq(2, 10, by = 1), max.em.its = 75
, seed = 9999)
toc()
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab,
K = seq(2, 10, by = 1),
max.em.its = 75, seed = 9999)
View(DFM2stm)
m
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab,
K = seq(2, 10, by = 1),
seed = 9999)
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab,
K = seq(3, 10, by = 1),
seed = 9999)
plot(IdealK)
DFM2stm <- convert(DFM, to = "stm")
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab,
K = seq(3, 10, by = 1),
seed = 9999)
plot(IdealK)
DFM
plan("default")
start_time_stm <- Sys.time()
nTopics <- seq(2,10,1)
many_models_stm <- data_frame(K = nTopics) %>%
mutate(topic_model = future_map(K, ~stm(dfm, K = ., verbose = TRUE)))
end_time_stm <- Sys.time() # 4 mins
heldout <- make.heldout(dfSparse)
heldout <- make.heldout(dfm)
k_result <- many_models_stm %>%
mutate(exclusivity        = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dfSparse),
eval_heldout       = map(topic_model, eval.heldout, heldout$missing),
residual           = map(topic_model, checkResiduals, dfSparse),
bound              = map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact              = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound             = bound + lfact,
iterations         = map_dbl(topic_model, function(x) length(x$convergence$bound)))
k_result <- many_models_stm %>%
mutate(exclusivity        = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dfm),
eval_heldout       = map(topic_model, eval.heldout, heldout$missing),
residual           = map(topic_model, checkResiduals, gfm),
bound              = map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact              = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound             = bound + lfact,
iterations         = map_dbl(topic_model, function(x) length(x$convergence$bound)))
heldout <- make.heldout(dfm)
k_result <- many_models_stm %>%
mutate(exclusivity        = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, dfm),
eval_heldout       = map(topic_model, eval.heldout, heldout$missing),
residual           = map(topic_model, checkResiduals, gfm),
bound              = map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact              = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound             = bound + lfact,
iterations         = map_dbl(topic_model, function(x) length(x$convergence$bound)))
View(heldout)
k_result %>%
transmute(K,
`Lower bound`         = lbound,
Residuals             = map_dbl(residual, "dispersion"),
`Semantic coherence`  = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x        = "K (number of topics)",
y        = NULL,
title    = "Model diagnostics by number of topics",
subtitle = "These diagnostics indicate that a good number of topics would be around 15")
library(stm)
library(furrr)
many_models <- tibble(K = c(2, 4, 6, 8, 10, 12)) %>%
mutate(topic_model = future_map(K, ~stm(dfm, K = .,
verbose = FALSE)))
threads_sparse <- dfm
many_models <- tibble::tibble(K = seq(5, 50, by = 1)) %>%
dplyr::mutate(topic_model = future_map(K, ~stm(threads_sparse, K = .,
verbose = FALSE)))
many_models <- tibble::tibble(K = seq(2, 15, by = 1)) %>%
dplyr::mutate(topic_model = future_map(K, ~stm(threads_sparse, K = .,
verbose = FALSE)))
heldout <- make.heldout(threads_sparse)
many_models <- data_frame(K = seq(5, 20, by = 5)) %>%
mutate(topic_model = future_map(K, ~stm(threads_sparse,
K = .,
verbose = FALSE)))
library(furrr)
many_models <- data_frame(K = seq(2, 15, by = 1)) %>%
mutate(topic_model = future_map(K, ~stm(threads_sparse,
K = .,
verbose = FALSE)))
dfm2stm <- convert(dfm, to = "stm")
mein.stm.idealK <- searchK(dfm2stm$documents, dfm2stm$vocab, K = seq(1, 12, by = 1), max.em.its = 75)
mein.stm.idealK <- searchK(dfm2stm$documents, dfm2stm$vocab, K = seq(2, 12, by = 1), max.em.its = 75)
View(dfm2stm)
mein.stm.idealK <- searchK(dfm2stm$documents, dfm2stm$vocab, K = seq(2, 5, by = 1), max.em.its = 75)
mein.stm.idealK <- searchK(dfm2stm$documents, dfm2stm$vocab, K = seq(2, 15, by = 1), max.em.its = 75)
mein.stm.idealK <- searchK(dfm$documents, dfm$vocab, K = seq(2, 15, by = 1), max.em.its = 75)
install.packages("stmCorrViz")
# clean current workspace
rm(list=ls(all=T))
library(stm)        # Package for sturctural topic modeling
library(igraph)     # Package for network analysis and visualisation
library(stmCorrViz) # Package for hierarchical correlation view of STMs
data <- read.csv("data/poliblogs2008.csv") # Download link: https://goo.gl/4ohgr4
data <- read.csv("poliblogs2008.csv") # Download link: https://goo.gl/4ohgr4
getwd()
setwd("D:/GitHub/learning-stm-master/")
data <- read.csv("data/poliblogs2008.csv") # Download link: https://goo.gl/4ohgr4
load("data/VignetteObjects.RData")         # Download link: https://goo.gl/xK17EQ
View(data)
# Stemming, stopword removal, etc. using textProcessor() function
processed <- textProcessor(data$documents, metadata=data)
# Structure and index for usage in the STM model. Ensure that object has no missing
# values. Remove low frequency words using 'lower.thresh' option. See ?prepDocuments
# for more information.
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
View(processed)
# The output will have object meta, documents, and vocab
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
# Another option is the manyTopics() function that performs model selection across
# separate STMs that each assume different number of topics. It works the same as
# selectModel(), except user specifies a range of numbers of topics that they want
# the model fitted for. For example, models with 5, 10, and 15 topics. Then, for
# each number of topics, selectModel() is run multiple times. The output is then
# processed through a function that takes a pareto dominant run of the model in
# terms of exclusivity and semantic coherence. If multiple runs are candidates
# (i.e., none weakly dominates the others), a single model run is randomly chosen
# from the set of undominated runs. Save plots as pdf files.
storage <- manyTopics(out$documents, out$vocab, K=c(2:4), prevalence=~rating+s(day),
data=meta, runs=10)
warnings()
# clean current workspace
rm(list=ls(all=T))
setwd("D:/GitHub/Latin_Text_Topic_Modeling/")
# clean current workspace
rm(list=ls(all=T))
library(textmineR)
library(igraph)
library(ggraph)
library(ggplot2)
library(tm)
library(udpipe)
library(stm)
prologus<-paste(scan(file ="files/01 prologus.txt",what='character'),collapse=" ")
historia_g<-paste(scan(file ="files/02 historia_g.txt",what='character'),collapse=" ")
recapitulatio<-paste(scan(file ="files/03 recapitulatio.txt",what='character'),collapse=" ")
historia_w<-paste(scan(file ="files/04 historia_w.txt",what='character'),collapse=" ")
historia_s<-paste(scan(file ="files/05 historia_s.txt",what='character'),collapse=" ")
prologus<-data.frame(texts=prologus)
historia_g<-data.frame(texts=historia_g)
recapitulatio<-data.frame(texts=recapitulatio)
historia_w<-data.frame(texts=historia_w)
historia_s<-data.frame(texts=historia_s)
prologus$book<-"01 Prologus"
historia_g$book<-"02 Historia Gothorum"
recapitulatio$book<-"03 Recapitulatio"
historia_w$book<-"04 Historia Wandalorum"
historia_s$book<-"05 Historia Suevorum"
historia<-rbind(prologus,historia_g,recapitulatio,historia_w,historia_s)
#historia$texts <- stripWhitespace(historia$texts)
historia$texts <- tolower(historia$texts)
historia$texts <- removePunctuation(historia$texts)
historia$texts <- removeNumbers(historia$texts)
# UDPipe annotation
#udmodel_latin <- udpipe_download_model(language = "latin-ittb")
#udmodel_latin <- udpipe_load_model(ud_model$file_model)
udmodel_latin <- udpipe_load_model(file = "latin-ittb-ud-2.5-191206.udpipe")
x <- udpipe_annotate(udmodel_latin, x = historia$texts, doc_id = historia$book, tagger = "default", parser = "default", trace = TRUE)
annotated_plots <- x
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
x <- as.data.frame(x)
annotated_plots <- x
library(tidyverse)
library(stm)
library(tidytext)
library(quanteda)
library(qdap)
library(furrr)
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
dfm2stm <- convert(dfm, to = "stm")
dfm2stm <- convert(dfm, to = "stm", docvars = docvars(dfm))
View(dfm2stm)
kresult.D.de.deepl <- searchK(documents = dfm2stm$documents, vocab = dfm2stm$vocab, data = dfm2stm$meta,
K = c(2, 3, 4, 5), init.type = "LDA")
plot.searchK(kresult.D.de.deepl)
kresult.D.de.deepl <- searchK(documents = dfm2stm$documents, vocab = dfm2stm$vocab, data = dfm2stm$meta,
K = c(2, 3, 4, 5), init.type = "LDA")
dtf <- subset(x, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "doc_id", term = "lemma")
head(dtf)
dtm <- document_term_matrix(x = dtf)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 4)
#Construct term-document matrix
mat <- readCorpus(dtm, type = c("slam"))
library(tm)
annotated_plots_clean <- annotated_plots %>%
mutate(lemma = str_to_lower(lemma)) %>%
filter(!upos %in% c("X", "SYM", "NUM", "PUNCT")) %>%
anti_join(stop_words, by = c("lemma" = "word"))
# Создание dfm из data frame UDPipe
annotated_plots_clean %>%
count(doc_id, lemma) %>%
cast_dfm(doc_id, lemma, n) -> dfm
#Construct term-document matrix
mat <- readCorpus(dfm, type = c("slam"))
#Construct term-document matrix
mat <- readCorpus(dtm)
processed <- prepDocuments(mat$documents, mat$vocab, lower.thresh = 5)
processed <- prepDocuments(mat$documents, mat$vocab, lower.thresh = 2)
ptm <- proc.time()
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
print(proc.time() - ptm)
print(proc.time() - ptm)
plot.searchK(kresult)
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
install.packages("geometry")
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
install.packages("Rtsne")
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
install.packages("rsvd")
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
#ptm <- proc.time()
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
library(Rtsne)
#ptm <- proc.time()
#Run searchK function
kresult <- searchK(processed$documents, processed$vocab, c(0))
class(dfm)
DFM2stm <- convert(dfm, to = "stm")
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab
, K = seq(4, 15, by = 1), max.em.its = 75
, cores = 4
, seed = 9999)
IdealK <- searchK(DFM2stm$documents, DFM2stm$vocab
, K = seq(2, 10, by = 1), max.em.its = 75
, seed = 9999)
docs <- DFM2stm$documents
vocab <- DFM2stm$vocab
meta <-DFM2stm$meta
IdealK <- searchK(ocuments, vocab
, K = seq(2, 10, by = 1), max.em.its = 75
, seed = 9999)
IdealK <- searchK(documents, vocab
, K = seq(2, 10, by = 1), max.em.its = 75
, seed = 9999)
IdealK <- searchK(docs, vocab
, K = seq(2, 10, by = 1), max.em.its = 75
, seed = 9999)
# clean current workspace
rm(list=ls(all=T))
